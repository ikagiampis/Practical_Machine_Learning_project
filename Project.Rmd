---
title: "Project - Practical Machine Learning Class"
author: "IK"
date: "August 21, 2015"
output: html_document
---
### Introduction

In this project I try to predict if 6 participants perform barbell correctly using sensors at the belt, arm, forearms and dumbbells. You can find the data at: http://groupware.les.inf.puc-rio.br/har

```{r}
library(caret)
library(dplyr)
library(rpart)
library(rpart.plot)
library(RColorBrewer)
library(rattle)
library(randomForest)
```

###ANALYSIS

At first I downloaded the data sets.

This code can't be execute in the Markdown file (Use this code to download the files fro the internet):

url <- 'https://d396qusza40orc.cloudfront.net/predmachlearn/pml-training.csv'
download.file(url, 'trainingData.csv')

url <- 'https://d396qusza40orc.cloudfront.net/predmachlearn/pml-testing.csv'
download.file(url, 'testingData.csv')


I have two data sets. The first data set is my training data while the second data set is my testing data.

```{r}
trainingData <- read.table("trainingData.csv", sep = ',', header = T)
testingData <- read.table("testingData.csv", sep = ',', header = T)
```

I am splitting my training data set on two subsets. I will use the first subset for training of the algorithm and the second for testing the algorithm before I apply my algorithm to the main test set.

```{r}
Training <- createDataPartition(y = trainingData$classe, p =0.6, list = FALSE)

forTraining <- trainingData[Training, ]

forTesting <- trainingData[-Training, ]
```

The dimension of the training subset is:
```{r}
dim(forTraining)
```

The dimension of the testing subset is:
```{r}
dim(forTesting)
```

### DATA TRANFORMATION AT THE TRAINING SET

First Transformation:

Some variables are zero or near zero. I removed these variables since they are not useful for building a model.

```{r}
#First Tranformation: Find all the variables that it is near zero
NearZeroVariables <- nearZeroVar(forTraining, saveMetrics=TRUE)

#Make the rownames as a column
NearZeroVariables$Index <-rownames(NearZeroVariables)

#Filter out all the variables that are near zero or zero
NoNearZeroVariables <- filter(NearZeroVariables, zeroVar == "FALSE", nzv == "FALSE")

#Take the names of the variables that are not near zero
NoNearZeroVarList = NoNearZeroVariables$Index

#New List
TrainingDF_T1 <- forTraining[NoNearZeroVarList]
```

The dimension of the training set is:

```{r}
dim(TrainingDF_T1)
```

Second Transformation:

I removed the row index (column 1) as is is not a varriable.

```{r}
#Removing the first column (row Index)
TrainingDF_T2 <- TrainingDF_T1[c(-1)]
dim(TrainingDF_T2)
```

The new DataFrame dimension is:

```{r}
dim(TrainingDF_T2)
```

Third Transformation:

Remove all the variables that have more that 60% NAs. I assume that if there are not a lot of data in the variables they will cause noise in the model.

```{r}
#Removing all the columns that have more than 60% na
TrainingDF_T3 <- TrainingDF_T2[ lapply(TrainingDF_T2, function(x) sum(is.na(x)) / length(x) ) < 0.6 ]
```

The new DataFrame dimension is:

```{r}
dim(TrainingDF_T3)
```


###DATA TRANSFORMATIONS AT THE TESTING SETS

I will do the same transformation as on the training set to my initial testing set.

```{r}
#Do the same transformation at the testing set by selecting the column names, keeping the column name classe

ColNameList <- colnames(TrainingDF_T3)
forTestingDF_Final = forTesting[ColNameList]
```

The dimension of the initial testing set is:

```{r}
dim(forTestingDF_Final)
```

Now I will transform my main testing set (It does not have the column 'classe')

```{r}
#Prepare the main testing data

TrainingDF_T4 <- select(TrainingDF_T3, -classe)

NoClasseColNameList <- colnames(TrainingDF_T4)

New_testingData <- testingData[NoClasseColNameList]
```

The dimension of the main testing set is:

```{r}
dim(New_testingData)
```

### BUILDING THE MODEL

##Decision Tree

```{r}
#########################################
#'''DECISION TREE'''
#########################################

DataFit1 <- rpart(classe ~ ., data=TrainingDF_T3, method="class")

predictionDataFit1 <-  predict(DataFit1, forTestingDF_Final, type = "class")

```

Decision Tree plot:

```{r}
fancyRpartPlot(DataFit1)
```

Here is the confusion matrix

```{r}
confusionMatrix(predictionDataFit1, forTestingDF_Final$classe)
```

The accuracy of Decision Tree model is: 0.87

###Random Forest
Since the Random forest correct the decision tress habit of overfitting to their training set (wiki), I expect better results...

```{r}
DataFit2 <- randomForest(classe ~. , data=TrainingDF_T3)

predictionDataFit2 <-  predict(DataFit2, forTestingDF_Final, type = "class")
```

Confusion Matrix:

```{r}
confusionMatrix(predictionDataFit2, forTestingDF_Final$classe)
```

The accuracy of this model is: 0.9991

###Conclusion

For the final Step I will use the Random Forest Model since it gives better prediction on the initial test set (The testing set that I know the outcome).


###Final Step

I am testing my model to the final testing set.

```{r}
#########################################
#'''Final Step'''
#########################################

DataFitFinal <- predict(DataFit1, New_testingData, type = "class")

pml_write_files = function(x){
  n = length(x)
  for(i in 1:n){
    filename = paste0("problem_id_",i,".txt")
    write.table(x[i],file=filename,quote=FALSE,row.names=FALSE,col.names=FALSE)
  }
}

pml_write_files(DataFitFinal)
```
